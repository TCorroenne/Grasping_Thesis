{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac02b048-a4e9-4136-bdf4-3f092314f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported everything\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import tensorboardX\n",
    "\n",
    "from PEGG_Net.utils.visualisation.gridshow import gridshow\n",
    "\n",
    "from PEGG_Net.utils.dataset_processing import evaluation\n",
    "\n",
    "from utility.data import get_custom_dataset\n",
    "from PEGG_Net.models import get_network\n",
    "import utility.io_processing as iop\n",
    "print(\"imported everything\")\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417d3214-8ca1-4817-820e-674279bffcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, device, val_data):  #, batches_per_epoch):\n",
    "    \"\"\"\n",
    "    Run validation.\n",
    "    :param net: Network\n",
    "    :param device: Torch device\n",
    "    :param val_data: Validation Dataset\n",
    "    :param batches_per_epoch: Number of batches to run\n",
    "    :return: Successes, Failures and Losses\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "\n",
    "    results = {\n",
    "        'correct': 0,\n",
    "        'failed': 0,\n",
    "        'loss': 0,\n",
    "        'losses': {\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ld = len(val_data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # batch_idx = 0\n",
    "        # while batch_idx < batches_per_epoch:\n",
    "        for x, y, didx, rot, zoom_factor in val_data:\n",
    "            # batch_idx += 1\n",
    "            # if batches_per_epoch is not None and batch_idx >= batches_per_epoch:\n",
    "                # break\n",
    "\n",
    "            xc = x.to(device)\n",
    "            yc = [yy.to(device) for yy in y]\n",
    "            lossd = net.compute_loss(xc, yc)\n",
    "\n",
    "            loss = lossd['loss']\n",
    "\n",
    "            results['loss'] += loss.item()/ld\n",
    "            for ln, l in lossd['losses'].items():\n",
    "                if ln not in results['losses']:\n",
    "                    results['losses'][ln] = 0\n",
    "                results['losses'][ln] += l.item()/ld\n",
    "\n",
    "            q_out, ang_out, w_out = iop.process_raw_output(lossd['pred']['pos'], lossd['pred']['cos'],\n",
    "                                                        lossd['pred']['sin'], lossd['pred']['width'])\n",
    "\n",
    "            s = evaluation.calculate_iou_match(q_out, ang_out,\n",
    "                                                val_data.dataset.get_gtbb(didx, rot, zoom_factor),\n",
    "                                                no_grasps=1,\n",
    "                                                grasp_width=w_out,\n",
    "                                                )\n",
    "\n",
    "            if s:\n",
    "                results['correct'] += 1\n",
    "            else:\n",
    "                results['failed'] += 1\n",
    "    # print(batch_idx, batches_per_epoch, results)\n",
    "    return results\n",
    "\n",
    "def train(epoch, net, device, train_data, optimizer, batches_per_epoch, vis=False):\n",
    "    \"\"\"\n",
    "    Run one training epoch\n",
    "    :param epoch: Current epoch\n",
    "    :param net: Network\n",
    "    :param device: Torch device\n",
    "    :param train_data: Training Dataset\n",
    "    :param optimizer: Optimizer\n",
    "    :param batches_per_epoch:  Data batches to train on\n",
    "    :param vis:  Visualise training progress\n",
    "    :return:  Average Losses for Epoch\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'loss': 0,\n",
    "        'losses': {\n",
    "        }\n",
    "    }\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    batch_idx = 0\n",
    "    # Use batches per epoch to make training on different sized datasets (cornell/jacquard) more equivalent.\n",
    "    while batch_idx < batches_per_epoch:\n",
    "        for x, y, _, _, _ in train_data:\n",
    "            batch_idx += 1\n",
    "            if batch_idx >= batches_per_epoch:\n",
    "                break\n",
    "\n",
    "            xc = x.to(device)\n",
    "            yc = [yy.to(device) for yy in y]\n",
    "            lossd = net.compute_loss(xc, yc)\n",
    "\n",
    "            loss = lossd['loss']\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                logging.info('Epoch: {}, Batch: {}, Loss: {:0.4f}'.format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "            results['loss'] += loss.item()\n",
    "            for ln, l in lossd['losses'].items():\n",
    "                if ln not in results['losses']:\n",
    "                    results['losses'][ln] = 0\n",
    "                results['losses'][ln] += l.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display the images\n",
    "            if vis:\n",
    "                imgs = []\n",
    "                n_img = min(4, x.shape[0])\n",
    "                for idx in range(n_img):\n",
    "                    imgs.extend([x[idx,].numpy().squeeze()] + [yi[idx,].numpy().squeeze() for yi in y] + [\n",
    "                        x[idx,].numpy().squeeze()] + [pc[idx,].detach().cpu().numpy().squeeze() for pc in lossd['pred'].values()])\n",
    "                gridshow('Display', imgs,\n",
    "                         [(xc.min().item(), xc.max().item()), (0.0, 1.0), (0.0, 1.0), (-1.0, 1.0), (0.0, 1.0)] * 2 * n_img,\n",
    "                         [cv2.COLORMAP_BONE] * 10 * n_img, 10)\n",
    "                cv2.waitKey(2)\n",
    "\n",
    "    results['loss'] /= batch_idx\n",
    "    for l in results['losses']:\n",
    "        results['losses'][l] /= batch_idx\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa7c648-9acb-4ca8-9d47-4391c368cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "state_dict = None\n",
    "network = \"peggnet\"\n",
    "input_size = 480\n",
    "max_width = 150\n",
    "dataset = \"cornell\"\n",
    "dataset_path = \"Cornell_dataset/\"\n",
    "use_depth = True\n",
    "use_rgb = True\n",
    "\n",
    "split = 0.9\n",
    "ds_rotate = 0.0\n",
    "image_wise = False\n",
    "random_seed = 10\n",
    "augment = False\n",
    "num_workers = 8\n",
    "\n",
    "lr = 0.001\n",
    "lr_step = \"40\"\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "batches_per_epoch = 1000\n",
    "\n",
    "description = \"\"\n",
    "outdir = \"output/models\"\n",
    "logdir = \"tensorboard/\"\n",
    "vis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a8ee48-9de8-4a51-9305-4f00f54d61a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Cornell Dataset...\n",
      "INFO:root:Done\n",
      "INFO:root:The size of the Cornell Dataset is: 100\n",
      "INFO:root:Number of training images: 90\n",
      "INFO:root:Number of validation images: 10\n",
      "INFO:root:Data augmentation: False\n",
      "INFO:root:Loading Network...\n",
      "INFO:root:Number of input channels: 4\n",
      "INFO:root:Using device: cuda\n",
      "INFO:root:Network Loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 480, 480]           1,152\n",
      "       BatchNorm2d-2         [-1, 32, 480, 480]              64\n",
      "              Mish-3         [-1, 32, 480, 480]               0\n",
      "Conv_Bn_Activation-4         [-1, 32, 480, 480]               0\n",
      "            Conv2d-5         [-1, 32, 480, 480]           1,024\n",
      "       BatchNorm2d-6         [-1, 32, 480, 480]              64\n",
      "              Mish-7         [-1, 32, 480, 480]               0\n",
      "Conv_Bn_Activation-8         [-1, 32, 480, 480]               0\n",
      "            Conv2d-9         [-1, 32, 480, 480]           9,216\n",
      "      BatchNorm2d-10         [-1, 32, 480, 480]              64\n",
      "             Mish-11         [-1, 32, 480, 480]               0\n",
      "Conv_Bn_Activation-12         [-1, 32, 480, 480]               0\n",
      "         ResBlock-13         [-1, 32, 480, 480]               0\n",
      "           Conv2d-14         [-1, 64, 240, 240]          18,432\n",
      "      BatchNorm2d-15         [-1, 64, 240, 240]             128\n",
      "             Mish-16         [-1, 64, 240, 240]               0\n",
      "Conv_Bn_Activation-17         [-1, 64, 240, 240]               0\n",
      "           Conv2d-18         [-1, 64, 240, 240]           4,096\n",
      "      BatchNorm2d-19         [-1, 64, 240, 240]             128\n",
      "             Mish-20         [-1, 64, 240, 240]               0\n",
      "Conv_Bn_Activation-21         [-1, 64, 240, 240]               0\n",
      "           Conv2d-22         [-1, 64, 240, 240]          36,864\n",
      "      BatchNorm2d-23         [-1, 64, 240, 240]             128\n",
      "             Mish-24         [-1, 64, 240, 240]               0\n",
      "Conv_Bn_Activation-25         [-1, 64, 240, 240]               0\n",
      "         ResBlock-26         [-1, 64, 240, 240]               0\n",
      "           Conv2d-27        [-1, 128, 120, 120]          73,728\n",
      "      BatchNorm2d-28        [-1, 128, 120, 120]             256\n",
      "             Mish-29        [-1, 128, 120, 120]               0\n",
      "Conv_Bn_Activation-30        [-1, 128, 120, 120]               0\n",
      "           Conv2d-31        [-1, 128, 120, 120]          16,384\n",
      "      BatchNorm2d-32        [-1, 128, 120, 120]             256\n",
      "             Mish-33        [-1, 128, 120, 120]               0\n",
      "Conv_Bn_Activation-34        [-1, 128, 120, 120]               0\n",
      "           Conv2d-35        [-1, 128, 120, 120]         147,456\n",
      "      BatchNorm2d-36        [-1, 128, 120, 120]             256\n",
      "             Mish-37        [-1, 128, 120, 120]               0\n",
      "Conv_Bn_Activation-38        [-1, 128, 120, 120]               0\n",
      "         ResBlock-39        [-1, 128, 120, 120]               0\n",
      "           Conv2d-40          [-1, 256, 60, 60]         294,912\n",
      "      BatchNorm2d-41          [-1, 256, 60, 60]             512\n",
      "             Mish-42          [-1, 256, 60, 60]               0\n",
      "Conv_Bn_Activation-43          [-1, 256, 60, 60]               0\n",
      "           Conv2d-44          [-1, 256, 60, 60]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 60, 60]             512\n",
      "             Mish-46          [-1, 256, 60, 60]               0\n",
      "Conv_Bn_Activation-47          [-1, 256, 60, 60]               0\n",
      "           Conv2d-48          [-1, 128, 60, 60]          32,768\n",
      "      BatchNorm2d-49          [-1, 128, 60, 60]             256\n",
      "             Mish-50          [-1, 128, 60, 60]               0\n",
      "Conv_Bn_Activation-51          [-1, 128, 60, 60]               0\n",
      "        MaxPool2d-52          [-1, 128, 60, 60]               0\n",
      "        MaxPool2d-53          [-1, 128, 60, 60]               0\n",
      "        MaxPool2d-54          [-1, 128, 60, 60]               0\n",
      "              SPP-55          [-1, 512, 60, 60]               0\n",
      "           Conv2d-56          [-1, 256, 60, 60]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 60, 60]             512\n",
      "             Mish-58          [-1, 256, 60, 60]               0\n",
      "Conv_Bn_Activation-59          [-1, 256, 60, 60]               0\n",
      "     PixelShuffle-60        [-1, 128, 120, 120]               0\n",
      "           Conv2d-61        [-1, 128, 120, 120]          16,384\n",
      "      BatchNorm2d-62        [-1, 128, 120, 120]             256\n",
      "             Mish-63        [-1, 128, 120, 120]               0\n",
      "Conv_Bn_Activation-64        [-1, 128, 120, 120]               0\n",
      "     PixelShuffle-65         [-1, 64, 240, 240]               0\n",
      "           Conv2d-66         [-1, 64, 240, 240]           4,096\n",
      "      BatchNorm2d-67         [-1, 64, 240, 240]             128\n",
      "             Mish-68         [-1, 64, 240, 240]               0\n",
      "Conv_Bn_Activation-69         [-1, 64, 240, 240]               0\n",
      "     PixelShuffle-70         [-1, 32, 480, 480]               0\n",
      "           Conv2d-71         [-1, 32, 480, 480]           1,024\n",
      "      BatchNorm2d-72         [-1, 32, 480, 480]              64\n",
      "             ReLU-73         [-1, 32, 480, 480]               0\n",
      "Conv_Bn_Activation-74         [-1, 32, 480, 480]               0\n",
      "           Conv2d-75          [-1, 1, 480, 480]             577\n",
      "           Conv2d-76          [-1, 1, 480, 480]             577\n",
      "           Conv2d-77          [-1, 1, 480, 480]             577\n",
      "           Conv2d-78          [-1, 1, 480, 480]             577\n",
      "================================================================\n",
      "Total params: 1,384,324\n",
      "Trainable params: 1,384,324\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.52\n",
      "Forward/backward pass size (MB): 1901.95\n",
      "Params size (MB): 5.28\n",
      "Estimated Total Size (MB): 1910.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr_step = [int(i) for i in lr_step.split(',')]\n",
    "# !!! USING CV2 VIS IN JUPYTER NOTEBOOK MAKE THE CORE CRASH\n",
    "# if vis:\n",
    "#     cv2.namedWindow('Display', cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "# Set-up output directories\n",
    "dt = datetime.datetime.now().strftime('%y%m%d_%H%M')\n",
    "net_desc = '{}_{}'.format(dt, '_'.join(description.split()))\n",
    "save_folder = os.path.join(outdir, net_desc)\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "tb = tensorboardX.SummaryWriter(os.path.join(logdir, net_desc))\n",
    "# Load Dataset\n",
    "logging.info('Loading {} Dataset...'.format(dataset.title()))\n",
    "Dataset = get_custom_dataset(dataset)\n",
    "\n",
    "train_dataset = Dataset(file_path=dataset_path,\n",
    "                            output_size=input_size, \n",
    "                            start=0.0, \n",
    "                            end=split, \n",
    "                            ds_rotate=ds_rotate,\n",
    "                            image_wise=image_wise,\n",
    "                            random_seed=random_seed,\n",
    "                            random_rotate=augment,\n",
    "                            random_zoom=augment,\n",
    "                            include_depth=use_depth, \n",
    "                            include_rgb=use_rgb,\n",
    "                            max_width=max_width)\n",
    "train_data = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_dataset = Dataset(file_path=dataset_path,\n",
    "                        output_size=input_size,\n",
    "                        start=split, \n",
    "                        end=1.0, \n",
    "                        ds_rotate=ds_rotate,\n",
    "                        image_wise=image_wise, \n",
    "                        random_seed=random_seed,\n",
    "                        random_rotate=False, \n",
    "                        random_zoom=False,\n",
    "                        include_depth=use_depth,\n",
    "                        include_rgb=use_rgb,\n",
    "                        max_width=max_width)\n",
    "val_data = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "logging.info('Done')\n",
    "logging.info('The size of the {} Dataset is: {}'.format(dataset.title(),\n",
    "                len(train_dataset)+len(val_dataset))\n",
    "            )\n",
    "logging.info('Number of training images: {}'.format(len(train_dataset)))\n",
    "logging.info('Number of validation images: {}'.format(len(val_dataset)))\n",
    "logging.info('Data augmentation: {}'.format(augment))\n",
    "\n",
    "# Load the network\n",
    "logging.info('Loading Network...')\n",
    "input_channels = 1*use_depth + 3*use_rgb\n",
    "logging.info(\"Number of input channels: {}\".format(input_channels))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info('Using device: {}'.format(device))\n",
    "\n",
    "network = get_network(network)\n",
    "net = network(input_channels=input_channels)\n",
    "if model is not None:\n",
    "    net = torch.load(model, map_location=device)\n",
    "elif state_dict is not None:\n",
    "    net.load_state_dict(torch.load(state_dict, map_location=device))\n",
    "net = net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "logging.info('Network Loaded')\n",
    "\n",
    "# Print model architecture.\n",
    "summary(net, (input_channels, input_size, input_size))\n",
    "f = open(os.path.join(save_folder, 'arch.txt'), 'w')\n",
    "sys.stdout = f\n",
    "summary(net, (input_channels, input_size, input_size))\n",
    "sys.stdout = sys.__stdout__\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f257d-527d-4104-a6c2-fa9c898051f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning Epoch 01\n",
      "INFO:root:Epoch: 1, Batch: 100, Loss: 0.0158\n",
      "INFO:root:Epoch: 1, Batch: 200, Loss: 0.0086\n",
      "INFO:root:Epoch: 1, Batch: 300, Loss: 0.0095\n",
      "INFO:root:Epoch: 1, Batch: 400, Loss: 0.0080\n",
      "INFO:root:Epoch: 1, Batch: 500, Loss: 0.0077\n",
      "INFO:root:Epoch: 1, Batch: 600, Loss: 0.0073\n",
      "INFO:root:Epoch: 1, Batch: 700, Loss: 0.0078\n",
      "INFO:root:Epoch: 1, Batch: 800, Loss: 0.0070\n",
      "INFO:root:Epoch: 1, Batch: 900, Loss: 0.0056\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 02\n",
      "INFO:root:Epoch: 2, Batch: 100, Loss: 0.0038\n",
      "INFO:root:Epoch: 2, Batch: 200, Loss: 0.0043\n",
      "INFO:root:Epoch: 2, Batch: 300, Loss: 0.0026\n",
      "INFO:root:Epoch: 2, Batch: 400, Loss: 0.0044\n",
      "INFO:root:Epoch: 2, Batch: 500, Loss: 0.0024\n",
      "INFO:root:Epoch: 2, Batch: 600, Loss: 0.0028\n",
      "INFO:root:Epoch: 2, Batch: 700, Loss: 0.0022\n",
      "INFO:root:Epoch: 2, Batch: 800, Loss: 0.0025\n",
      "INFO:root:Epoch: 2, Batch: 900, Loss: 0.0009\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 03\n",
      "INFO:root:Epoch: 3, Batch: 100, Loss: 0.0024\n",
      "INFO:root:Epoch: 3, Batch: 200, Loss: 0.0016\n",
      "INFO:root:Epoch: 3, Batch: 300, Loss: 0.0018\n",
      "INFO:root:Epoch: 3, Batch: 400, Loss: 0.0012\n",
      "INFO:root:Epoch: 3, Batch: 500, Loss: 0.0010\n",
      "INFO:root:Epoch: 3, Batch: 600, Loss: 0.0012\n",
      "INFO:root:Epoch: 3, Batch: 700, Loss: 0.0014\n",
      "INFO:root:Epoch: 3, Batch: 800, Loss: 0.0016\n",
      "INFO:root:Epoch: 3, Batch: 900, Loss: 0.0012\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 04\n",
      "INFO:root:Epoch: 4, Batch: 100, Loss: 0.0010\n",
      "INFO:root:Epoch: 4, Batch: 200, Loss: 0.0009\n",
      "INFO:root:Epoch: 4, Batch: 300, Loss: 0.0013\n",
      "INFO:root:Epoch: 4, Batch: 400, Loss: 0.0014\n",
      "INFO:root:Epoch: 4, Batch: 500, Loss: 0.0009\n",
      "INFO:root:Epoch: 4, Batch: 600, Loss: 0.0011\n",
      "INFO:root:Epoch: 4, Batch: 700, Loss: 0.0009\n",
      "INFO:root:Epoch: 4, Batch: 800, Loss: 0.0010\n",
      "INFO:root:Epoch: 4, Batch: 900, Loss: 0.0009\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 05\n",
      "INFO:root:Epoch: 5, Batch: 100, Loss: 0.0007\n",
      "INFO:root:Epoch: 5, Batch: 200, Loss: 0.0007\n",
      "INFO:root:Epoch: 5, Batch: 300, Loss: 0.0007\n",
      "INFO:root:Epoch: 5, Batch: 400, Loss: 0.0009\n",
      "INFO:root:Epoch: 5, Batch: 500, Loss: 0.0008\n",
      "INFO:root:Epoch: 5, Batch: 600, Loss: 0.0008\n",
      "INFO:root:Epoch: 5, Batch: 700, Loss: 0.0006\n",
      "INFO:root:Epoch: 5, Batch: 800, Loss: 0.0007\n",
      "INFO:root:Epoch: 5, Batch: 900, Loss: 0.0011\n",
      "INFO:root:Validating...\n",
      "INFO:root:7/10 = 0.700000\n",
      "INFO:root:Beginning Epoch 06\n",
      "INFO:root:Epoch: 6, Batch: 100, Loss: 0.0008\n",
      "INFO:root:Epoch: 6, Batch: 200, Loss: 0.0010\n",
      "INFO:root:Epoch: 6, Batch: 300, Loss: 0.0006\n",
      "INFO:root:Epoch: 6, Batch: 400, Loss: 0.0005\n",
      "INFO:root:Epoch: 6, Batch: 500, Loss: 0.0009\n",
      "INFO:root:Epoch: 6, Batch: 600, Loss: 0.0004\n",
      "INFO:root:Epoch: 6, Batch: 700, Loss: 0.0005\n",
      "INFO:root:Epoch: 6, Batch: 800, Loss: 0.0007\n",
      "INFO:root:Epoch: 6, Batch: 900, Loss: 0.0005\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 07\n",
      "INFO:root:Epoch: 7, Batch: 100, Loss: 0.0009\n",
      "INFO:root:Epoch: 7, Batch: 200, Loss: 0.0005\n",
      "INFO:root:Epoch: 7, Batch: 300, Loss: 0.0003\n",
      "INFO:root:Epoch: 7, Batch: 400, Loss: 0.0007\n",
      "INFO:root:Epoch: 7, Batch: 500, Loss: 0.0007\n",
      "INFO:root:Epoch: 7, Batch: 600, Loss: 0.0008\n",
      "INFO:root:Epoch: 7, Batch: 700, Loss: 0.0004\n",
      "INFO:root:Epoch: 7, Batch: 800, Loss: 0.0005\n",
      "INFO:root:Epoch: 7, Batch: 900, Loss: 0.0004\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 08\n",
      "INFO:root:Epoch: 8, Batch: 100, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 200, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 300, Loss: 0.0005\n",
      "INFO:root:Epoch: 8, Batch: 400, Loss: 0.0006\n",
      "INFO:root:Epoch: 8, Batch: 500, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 600, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 700, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 800, Loss: 0.0004\n",
      "INFO:root:Epoch: 8, Batch: 900, Loss: 0.0005\n",
      "INFO:root:Validating...\n",
      "INFO:root:7/10 = 0.700000\n",
      "INFO:root:Beginning Epoch 09\n",
      "INFO:root:Epoch: 9, Batch: 100, Loss: 0.0005\n",
      "INFO:root:Epoch: 9, Batch: 200, Loss: 0.0003\n",
      "INFO:root:Epoch: 9, Batch: 300, Loss: 0.0004\n",
      "INFO:root:Epoch: 9, Batch: 400, Loss: 0.0004\n",
      "INFO:root:Epoch: 9, Batch: 500, Loss: 0.0003\n",
      "INFO:root:Epoch: 9, Batch: 600, Loss: 0.0005\n",
      "INFO:root:Epoch: 9, Batch: 700, Loss: 0.0004\n",
      "INFO:root:Epoch: 9, Batch: 800, Loss: 0.0004\n",
      "INFO:root:Epoch: 9, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:6/10 = 0.600000\n",
      "INFO:root:Beginning Epoch 10\n",
      "INFO:root:Epoch: 10, Batch: 100, Loss: 0.0003\n",
      "INFO:root:Epoch: 10, Batch: 200, Loss: 0.0004\n",
      "INFO:root:Epoch: 10, Batch: 300, Loss: 0.0005\n",
      "INFO:root:Epoch: 10, Batch: 400, Loss: 0.0005\n",
      "INFO:root:Epoch: 10, Batch: 500, Loss: 0.0004\n",
      "INFO:root:Epoch: 10, Batch: 600, Loss: 0.0004\n",
      "INFO:root:Epoch: 10, Batch: 700, Loss: 0.0003\n",
      "INFO:root:Epoch: 10, Batch: 800, Loss: 0.0003\n",
      "INFO:root:Epoch: 10, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:6/10 = 0.600000\n",
      "INFO:root:Beginning Epoch 11\n",
      "INFO:root:Epoch: 11, Batch: 100, Loss: 0.0003\n",
      "INFO:root:Epoch: 11, Batch: 200, Loss: 0.0003\n",
      "INFO:root:Epoch: 11, Batch: 300, Loss: 0.0004\n",
      "INFO:root:Epoch: 11, Batch: 400, Loss: 0.0003\n",
      "INFO:root:Epoch: 11, Batch: 500, Loss: 0.0004\n",
      "INFO:root:Epoch: 11, Batch: 600, Loss: 0.0003\n",
      "INFO:root:Epoch: 11, Batch: 700, Loss: 0.0004\n",
      "INFO:root:Epoch: 11, Batch: 800, Loss: 0.0004\n",
      "INFO:root:Epoch: 11, Batch: 900, Loss: 0.0044\n",
      "INFO:root:Validating...\n",
      "INFO:root:6/10 = 0.600000\n",
      "INFO:root:Beginning Epoch 12\n",
      "INFO:root:Epoch: 12, Batch: 100, Loss: 0.0008\n",
      "INFO:root:Epoch: 12, Batch: 200, Loss: 0.0006\n",
      "INFO:root:Epoch: 12, Batch: 300, Loss: 0.0003\n",
      "INFO:root:Epoch: 12, Batch: 400, Loss: 0.0005\n",
      "INFO:root:Epoch: 12, Batch: 500, Loss: 0.0009\n",
      "INFO:root:Epoch: 12, Batch: 600, Loss: 0.0003\n",
      "INFO:root:Epoch: 12, Batch: 700, Loss: 0.0003\n",
      "INFO:root:Epoch: 12, Batch: 800, Loss: 0.0003\n",
      "INFO:root:Epoch: 12, Batch: 900, Loss: 0.0003\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 13\n",
      "INFO:root:Epoch: 13, Batch: 100, Loss: 0.0002\n",
      "INFO:root:Epoch: 13, Batch: 200, Loss: 0.0003\n",
      "INFO:root:Epoch: 13, Batch: 300, Loss: 0.0002\n",
      "INFO:root:Epoch: 13, Batch: 400, Loss: 0.0002\n",
      "INFO:root:Epoch: 13, Batch: 500, Loss: 0.0003\n",
      "INFO:root:Epoch: 13, Batch: 600, Loss: 0.0003\n",
      "INFO:root:Epoch: 13, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 13, Batch: 800, Loss: 0.0002\n",
      "INFO:root:Epoch: 13, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 14\n",
      "INFO:root:Epoch: 14, Batch: 100, Loss: 0.0003\n",
      "INFO:root:Epoch: 14, Batch: 200, Loss: 0.0003\n",
      "INFO:root:Epoch: 14, Batch: 300, Loss: 0.0002\n",
      "INFO:root:Epoch: 14, Batch: 400, Loss: 0.0002\n",
      "INFO:root:Epoch: 14, Batch: 500, Loss: 0.0002\n",
      "INFO:root:Epoch: 14, Batch: 600, Loss: 0.0002\n",
      "INFO:root:Epoch: 14, Batch: 700, Loss: 0.0003\n",
      "INFO:root:Epoch: 14, Batch: 800, Loss: 0.0003\n",
      "INFO:root:Epoch: 14, Batch: 900, Loss: 0.0003\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 15\n",
      "INFO:root:Epoch: 15, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 15, Batch: 200, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 300, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 400, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 500, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 600, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 15, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 15, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 16\n",
      "INFO:root:Epoch: 16, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 16, Batch: 200, Loss: 0.0002\n",
      "INFO:root:Epoch: 16, Batch: 300, Loss: 0.0003\n",
      "INFO:root:Epoch: 16, Batch: 400, Loss: 0.0008\n",
      "INFO:root:Epoch: 16, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 16, Batch: 600, Loss: 0.0002\n",
      "INFO:root:Epoch: 16, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 16, Batch: 800, Loss: 0.0003\n",
      "INFO:root:Epoch: 16, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 17\n",
      "INFO:root:Epoch: 17, Batch: 100, Loss: 0.0002\n",
      "INFO:root:Epoch: 17, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 17, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 17, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 17, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 17, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 17, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 17, Batch: 800, Loss: 0.0002\n",
      "INFO:root:Epoch: 17, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 18\n",
      "INFO:root:Epoch: 18, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 18, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 18, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 18, Batch: 400, Loss: 0.0002\n",
      "INFO:root:Epoch: 18, Batch: 500, Loss: 0.0002\n",
      "INFO:root:Epoch: 18, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 18, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 18, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 18, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 19\n",
      "INFO:root:Epoch: 19, Batch: 100, Loss: 0.0002\n",
      "INFO:root:Epoch: 19, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 19, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 20\n",
      "INFO:root:Epoch: 20, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 20, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 21\n",
      "INFO:root:Epoch: 21, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 21, Batch: 200, Loss: 0.0002\n",
      "INFO:root:Epoch: 21, Batch: 300, Loss: 0.0002\n",
      "INFO:root:Epoch: 21, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 21, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 21, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 21, Batch: 700, Loss: 0.0003\n",
      "INFO:root:Epoch: 21, Batch: 800, Loss: 0.0005\n",
      "INFO:root:Epoch: 21, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:5/10 = 0.500000\n",
      "INFO:root:Beginning Epoch 22\n",
      "INFO:root:Epoch: 22, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 300, Loss: 0.0002\n",
      "INFO:root:Epoch: 22, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 22, Batch: 800, Loss: 0.0002\n",
      "INFO:root:Epoch: 22, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:2/10 = 0.200000\n",
      "INFO:root:Beginning Epoch 23\n",
      "INFO:root:Epoch: 23, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 23, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 23, Batch: 900, Loss: 0.0000\n",
      "INFO:root:Validating...\n",
      "INFO:root:2/10 = 0.200000\n",
      "INFO:root:Beginning Epoch 24\n",
      "INFO:root:Epoch: 24, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 24, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:2/10 = 0.200000\n",
      "INFO:root:Beginning Epoch 25\n",
      "INFO:root:Epoch: 25, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 25, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:2/10 = 0.200000\n",
      "INFO:root:Beginning Epoch 26\n",
      "INFO:root:Epoch: 26, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 26, Batch: 900, Loss: 0.0002\n",
      "INFO:root:Validating...\n",
      "INFO:root:2/10 = 0.200000\n",
      "INFO:root:Beginning Epoch 27\n",
      "INFO:root:Epoch: 27, Batch: 100, Loss: 0.0000\n",
      "INFO:root:Epoch: 27, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 27, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 27, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 27, Batch: 500, Loss: 0.0000\n",
      "INFO:root:Epoch: 27, Batch: 600, Loss: 0.0002\n",
      "INFO:root:Epoch: 27, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 27, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 27, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 28\n",
      "INFO:root:Epoch: 28, Batch: 100, Loss: 0.0000\n",
      "INFO:root:Epoch: 28, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 300, Loss: 0.0000\n",
      "INFO:root:Epoch: 28, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 700, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 800, Loss: 0.0001\n",
      "INFO:root:Epoch: 28, Batch: 900, Loss: 0.0000\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 29\n",
      "INFO:root:Epoch: 29, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 29, Batch: 700, Loss: 0.0002\n",
      "INFO:root:Epoch: 29, Batch: 800, Loss: 0.0000\n",
      "INFO:root:Epoch: 29, Batch: 900, Loss: 0.0001\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 30\n",
      "INFO:root:Epoch: 30, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 30, Batch: 200, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 300, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 400, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 500, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 30, Batch: 700, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 800, Loss: 0.0000\n",
      "INFO:root:Epoch: 30, Batch: 900, Loss: 0.0000\n",
      "INFO:root:Validating...\n",
      "INFO:root:4/10 = 0.400000\n",
      "INFO:root:Beginning Epoch 31\n",
      "INFO:root:Epoch: 31, Batch: 100, Loss: 0.0001\n",
      "INFO:root:Epoch: 31, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 31, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 31, Batch: 400, Loss: 0.0001\n",
      "INFO:root:Epoch: 31, Batch: 500, Loss: 0.0000\n",
      "INFO:root:Epoch: 31, Batch: 600, Loss: 0.0001\n",
      "INFO:root:Epoch: 31, Batch: 700, Loss: 0.0000\n",
      "INFO:root:Epoch: 31, Batch: 800, Loss: 0.0000\n",
      "INFO:root:Epoch: 31, Batch: 900, Loss: 0.0000\n",
      "INFO:root:Validating...\n",
      "INFO:root:3/10 = 0.300000\n",
      "INFO:root:Beginning Epoch 32\n",
      "INFO:root:Epoch: 32, Batch: 100, Loss: 0.0000\n",
      "INFO:root:Epoch: 32, Batch: 200, Loss: 0.0001\n",
      "INFO:root:Epoch: 32, Batch: 300, Loss: 0.0001\n",
      "INFO:root:Epoch: 32, Batch: 400, Loss: 0.0002\n",
      "INFO:root:Epoch: 32, Batch: 500, Loss: 0.0001\n",
      "INFO:root:Epoch: 34, Batch: 600, Loss: 0.0000\n",
      "INFO:root:Epoch: 34, Batch: 700, Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     10\u001b[0m         param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m---> 12\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Log training losses to tensorboard\u001b[39;00m\n\u001b[1;32m     15\u001b[0m tb\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss/train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, train_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], epoch)\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, net, device, train_data, optimizer, batches_per_epoch, vis)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batches_per_epoch:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m xc \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m yc \u001b[38;5;241m=\u001b[39m [yy\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m yy \u001b[38;5;129;01min\u001b[39;00m y]\n\u001b[1;32m     89\u001b[0m lossd \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mcompute_loss(xc, yc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_iou = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    logging.info('Beginning Epoch {:02d}'.format(epoch))\n",
    "\n",
    "    if epoch in lr_step:\n",
    "        print(lr_step)\n",
    "        lr = lr * (0.1 ** (lr_step.index(epoch) + 1))\n",
    "        print('Drop LR to', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    train_results = train(\n",
    "        epoch, net, device, train_data, optimizer, batches_per_epoch, vis=vis\n",
    "    )\n",
    "\n",
    "    # Log training losses to tensorboard\n",
    "    tb.add_scalar('loss/train_loss', train_results['loss'], epoch)\n",
    "    for n, l in train_results['losses'].items():\n",
    "        tb.add_scalar('train_loss/' + n, l, epoch)\n",
    "\n",
    "    # Run Validation\n",
    "    logging.info('Validating...')\n",
    "    test_results = validate(net, device, val_data)  #, val_batches)\n",
    "    logging.info('%d/%d = %f' % (test_results['correct'],\n",
    "                                 test_results['correct'] + test_results['failed'],\n",
    "                                 test_results['correct']/(test_results['correct']+test_results['failed'])\n",
    "                                ))\n",
    "\n",
    "    # Log validation results to tensorbaord\n",
    "    tb.add_scalar(\n",
    "        'loss/IOU',\n",
    "        test_results['correct'] / (test_results['correct'] + test_results['failed']),\n",
    "        epoch\n",
    "    )\n",
    "    tb.add_scalar('loss/val_loss', test_results['loss'], epoch)\n",
    "    for n, l in test_results['losses'].items():\n",
    "        tb.add_scalar('val_loss/' + n, l, epoch)\n",
    "\n",
    "    # Save best performing network\n",
    "    iou = test_results['correct'] / (test_results['correct'] + test_results['failed'])\n",
    "    if iou > best_iou or epoch == 0 or (epoch % 10) == 0:\n",
    "        torch.save(\n",
    "            net,\n",
    "            os.path.join(save_folder, 'epoch_%02d_iou_%0.2f' % (epoch, iou))\n",
    "        )\n",
    "        torch.save(net.state_dict(), os.path.join(save_folder, 'epoch_%02d_iou_%0.2f_statedict.pt' % (epoch, iou)))\n",
    "        best_iou = iou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peggnet2",
   "language": "python",
   "name": "peggnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
